Chapter 07: Time series regression models
================
maurogm
2022-12-28

-   [Link](#link)
-   [The linear model](#the-linear-model)
    -   [Assumptions](#assumptions)
    -   [Evaluating the regression
        model](#evaluating-the-regression-model)
-   [Some useful predictors](#some-useful-predictors)
    -   [Términos de Fourier](#términos-de-fourier)
-   [Selecting predictors](#selecting-predictors)
-   [Forecasting with regression](#forecasting-with-regression)

En este capítulo se va a estudiar el problema de predecir una serie de
tiempo usando otras series de tiempo como predictores. Por ejemplo,
predecir el consumo eléctrico usando la temperatura como predictor.

## Link

<https://otexts.com/fpp3/regression.html>

## The linear model

Introduce el modelo lineal. Siguiendo con el framekowrk de los tsibbles,
mables, fables, etc., el modo de fittearlo es con la función `TSLM`. La
salida luego se puede ver con la función `report`:

``` r
consumption_model_mable <- us_change %>%
  model(TSLM(Consumption ~ Income))
consumption_model_mable %>% report()
```

    ## Series: Consumption 
    ## Model: TSLM 
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -2.58236 -0.27777  0.01862  0.32330  1.42229 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.54454    0.05403  10.079  < 2e-16 ***
    ## Income       0.27183    0.04673   5.817  2.4e-08 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.5905 on 196 degrees of freedom
    ## Multiple R-squared: 0.1472,  Adjusted R-squared: 0.1429
    ## F-statistic: 33.84 on 1 and 196 DF, p-value: 2.4022e-08

El resultado que arroja es básicamente el mismo que usando `lm` y
`summary`:

``` r
lm_clasico <- lm(Consumption ~ Income, data = us_change)
lm_clasico %>%
  summary()
```

    ## 
    ## Call:
    ## lm(formula = Consumption ~ Income, data = us_change)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -2.58236 -0.27777  0.01862  0.32330  1.42229 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.54454    0.05403  10.079  < 2e-16 ***
    ## Income       0.27183    0.04673   5.817  2.4e-08 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.5905 on 196 degrees of freedom
    ## Multiple R-squared:  0.1472, Adjusted R-squared:  0.1429 
    ## F-statistic: 33.84 on 1 and 196 DF,  p-value: 2.402e-08

### Assumptions

Pego la sección de supuestos porque es concisa y siempre está bueno
tenerlos a mano:

    First, we assume that the model is a reasonable approximation to reality;
    that is, the relationship between the forecast variable and the predictor
    variables satisfies this linear equation.

    Second, we make the following assumptions about the errors:

       - they have mean zero; otherwise the forecasts will be systematically biased.
       - they are not autocorrelated; otherwise the forecasts will be inefficient, as there is more information in the data that can be exploited.
       - they are unrelated to the predictor variables; otherwise there would be more information that should be included in the systematic part of the model.

    It is also useful to have the errors being normally distributed with a constant variance σ2
    in order to easily produce prediction intervals.

### Evaluating the regression model

Acá habla del análisis de los residuos para evaluar la salud del modelo.
Parecido a los que se hace con `plot(lm)`.

Es interesante un patrón que sugiere para detectar correlaciones
espurias que se dan cuando dos series que no tienen nada que ver tienen
una misma tendencia. En el ejemplo, modela la cantidad de pasajeros de
avión en Australia con la producción de arroz en Guinea. Dos cosas que
no tienen nada que ver, pero que tienden a aumentar con el paso del
tiempo:

``` r
fit <- aus_airpassengers %>%
  filter(Year <= 2011) %>%
  left_join(guinea_rice, by = "Year") %>%
  model(TSLM(Passengers ~ Production))
report(fit)
```

    ## Series: Passengers 
    ## Model: TSLM 
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -5.9448 -1.8917 -0.3272  1.8620 10.4210 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)   -7.493      1.203  -6.229 2.25e-07 ***
    ## Production    40.288      1.337  30.135  < 2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 3.239 on 40 degrees of freedom
    ## Multiple R-squared: 0.9578,  Adjusted R-squared: 0.9568
    ## F-statistic: 908.1 on 1 and 40 DF, p-value: < 2.22e-16

El R2 es altísimo. Pero cuando analizamos los residuos, vemos que los
mismos están altamente autocorrelacionados, lo que indica que nuestros
datos tienen una tendencia que no está capturada por el modelo. Esto, en
conjunción con el R2 alto, es un fuerte indicio de que estamos ante una
correlación espuria.

``` r
fit %>% gg_tsresiduals()
```

![](/workspaces/fpp/rendered_files/ch_07_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

## Some useful predictors

Dentro de un modelo que usa `TSLM` se le puede pasar a la fórmula las
funciones `trend` y `season`, para que cree automáticamente predictores
de tendencia y estacionalidad. El de tendencia crea un parámetro para la
variable `t`, y el de estacionalidad crea una dummy variable para cada
periodo de la serie.

Por ejemplo, para el modelo de producción de cerveza:

``` r
recent_production <- aus_production %>%
  filter(year(Quarter) >= 1992)
fit_beer <- recent_production %>%
  model(TSLM(Beer ~ trend() + season()))
report(fit_beer)
```

    ## Series: Beer 
    ## Model: TSLM 
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -42.9029  -7.5995  -0.4594   7.9908  21.7895 
    ## 
    ## Coefficients:
    ##                Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)   441.80044    3.73353 118.333  < 2e-16 ***
    ## trend()        -0.34027    0.06657  -5.111 2.73e-06 ***
    ## season()year2 -34.65973    3.96832  -8.734 9.10e-13 ***
    ## season()year3 -17.82164    4.02249  -4.430 3.45e-05 ***
    ## season()year4  72.79641    4.02305  18.095  < 2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 12.23 on 69 degrees of freedom
    ## Multiple R-squared: 0.9243,  Adjusted R-squared: 0.9199
    ## F-statistic: 210.7 on 4 and 69 DF, p-value: < 2.22e-16

Graficando como series:

``` r
augment(fit_beer) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Beer, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  scale_colour_manual(
    values = c(Data = "black", Fitted = "#D55E00")
  ) +
  labs(
    y = "Megalitres",
    title = "Australian quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Series"))
```

![](/workspaces/fpp/rendered_files/ch_07_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

Graficando como scatterplot:

``` r
augment(fit_beer) %>%
  ggplot(aes(
    x = Beer, y = .fitted,
    colour = factor(quarter(Quarter))
  )) +
  geom_point() +
  labs(
    y = "Fitted", x = "Actual values",
    title = "Australian quarterly beer production"
  ) +
  geom_abline(intercept = 0, slope = 1) +
  guides(colour = guide_legend(title = "Quarter"))
```

![](/workspaces/fpp/rendered_files/ch_07_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

### Términos de Fourier

En vezde usar dummy variables para la estacionalidad, se puede usar
términos de la serie de Fourier para capturar la estacionalidad. Esto es
especialmente útil cuando los períodos son muchos (por ejemplo 52
semanas en un año), dado que a menudo alcanza con incluir unos pocos
términos de Fourier para capturar la estacionalidad, con lo que tenemos
la misma performance con un modelo de menor dimensión.

Para el ejemplo de la cerveza:

``` r
fourier_beer <- recent_production %>%
  model(TSLM(Beer ~ trend() + fourier(K = 2)))
report(fourier_beer)
```

    ## Series: Beer 
    ## Model: TSLM 
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -42.9029  -7.5995  -0.4594   7.9908  21.7895 
    ## 
    ## Coefficients:
    ##                     Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)        446.87920    2.87321 155.533  < 2e-16 ***
    ## trend()             -0.34027    0.06657  -5.111 2.73e-06 ***
    ## fourier(K = 2)C1_4   8.91082    2.01125   4.430 3.45e-05 ***
    ## fourier(K = 2)S1_4 -53.72807    2.01125 -26.714  < 2e-16 ***
    ## fourier(K = 2)C2_4 -13.98958    1.42256  -9.834 9.26e-15 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 12.23 on 69 degrees of freedom
    ## Multiple R-squared: 0.9243,  Adjusted R-squared: 0.9199
    ## F-statistic: 210.7 on 4 and 69 DF, p-value: < 2.22e-16

## Selecting predictors

Introduce como métricas de capacidad predictiva: - R2 ajustado. -
Cross-validation. - AIC. - AICc. - BIC.

Todos estos (y algunas cosas más) son calculados por
`fabletools::glance()`:

``` r
fit_consMR <- us_change %>%
  model(tslm = TSLM(Consumption ~ Income + Production +
    Unemployment + Savings))
glance(fit_consMR) %>%
  select(adj_r_squared, CV, AIC, AICc, BIC)
```

    ## # A tibble: 1 × 5
    ##   adj_r_squared    CV   AIC  AICc   BIC
    ##           <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1         0.763 0.104 -457. -456. -437.

## Forecasting with regression

Si querés forecastear con un modelo de regresión, tenés que aportar los
valores de los predictores para el período de forecasteo, que
normalmente no tenés. Si justo tu modelo se basa en eventos de
calendario (por ejemplo, si sólo usa features de drift y
estacionalidad), entonces no tenés problema. Pero si no, hay que estimar
los valores de los predictores: ya sea con otros forecasts, o trabajando
bajo distintos escenarios.

``` r
fit_consBest <- us_change %>%
  model(
    lm = TSLM(Consumption ~ Income + Savings + Unemployment)
  )
future_scenarios <- fabletools::scenarios(
  Increase = new_data(us_change, 4) %>%
    mutate(Income = 1, Savings = 0.5, Unemployment = 0),
  Decrease = new_data(us_change, 4) %>%
    mutate(Income = -1, Savings = -0.5, Unemployment = 0),
  names_to = "Scenario"
)

fc <- forecast(fit_consBest, new_data = future_scenarios)

us_change %>%
  autoplot(Consumption) +
  autolayer(fc) +
  labs(title = "US consumption", y = "% change")
```

![](/workspaces/fpp/rendered_files/ch_07_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

**Modelo con predictores laggeados**: Si queremos poder predecir, tal
vez podemos armar un modelo que use predictores laggeados. De este modo,
al momento realizar una predicción ya vamos a tener disponibles los
valores de los predictores que necesitamos. \#\# Nonlinear regression

Primero habla de regresiones lineales sobre parámetros transformados.

Después introduce las regresiones no lineales, en particular la opción
de usar una función lineal a trozos.

Da el ejemplo de cómo usar una tendencia lienal a trozos:

``` r
boston_men <- boston_marathon %>%
  filter(Year >= 1924) %>%
  filter(Event == "Men's open division") %>%
  mutate(Minutes = as.numeric(Time)/60)
```

*Nota*: Dentro de `model()` puedo fitear varios modelos a la vez:

``` r
fit_trends <- boston_men %>%
  model(
    linear = TSLM(Minutes ~ trend()),
    exponential = TSLM(log(Minutes) ~ trend()),
    piecewise = TSLM(Minutes ~ trend(knots = c(1950, 1980)))
  )
fc_trends <- fit_trends %>% forecast(h = 10)
fc_trends %>% head(15)
```

    ## # A fable: 15 x 5 [1Y]
    ## # Key:     Event, .model [2]
    ##    Event               .model       Year           Minutes .mean
    ##    <fct>               <chr>       <dbl>            <dist> <dbl>
    ##  1 Men's open division linear       2020        N(123, 22)  123.
    ##  2 Men's open division linear       2021        N(123, 22)  123.
    ##  3 Men's open division linear       2022        N(122, 22)  122.
    ##  4 Men's open division linear       2023        N(122, 22)  122.
    ##  5 Men's open division linear       2024        N(122, 22)  122.
    ##  6 Men's open division linear       2025        N(121, 22)  121.
    ##  7 Men's open division linear       2026        N(121, 22)  121.
    ##  8 Men's open division linear       2027        N(121, 22)  121.
    ##  9 Men's open division linear       2028        N(120, 22)  120.
    ## 10 Men's open division linear       2029        N(120, 22)  120.
    ## 11 Men's open division exponential  2020 t(N(4.8, 0.0011))  124.
    ## 12 Men's open division exponential  2021 t(N(4.8, 0.0011))  123.
    ## 13 Men's open division exponential  2022 t(N(4.8, 0.0011))  123.
    ## 14 Men's open division exponential  2023 t(N(4.8, 0.0011))  123.
    ## 15 Men's open division exponential  2024 t(N(4.8, 0.0011))  123.

``` r
boston_men %>%
  autoplot(Minutes) +
  geom_line(data = fitted(fit_trends),
            aes(y = .fitted, colour = .model)) +
  autolayer(fc_trends, alpha = 0.5, level = 95) +
  labs(y = "Minutes",
       title = "Boston marathon winning times")
```

![](/workspaces/fpp/rendered_files/ch_07_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->
